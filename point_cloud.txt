In the last video,
you learned how we can use geometry
to create occupancy grid maps from 3D point clouds.
Now, you will learn an alternative approach using deep learning to solve this problem.
In many fields, especially in computer vision,
there has been a transformation
from manually derived mathematical models to deep learning-based models.
For example, several years ago,
the Hough transformation was one state-of-the-art method
to detect lane boundaries in images.
The image is preprocessed using mathematical filters,
the brightness of each pixel is evaluated,
and straight lines are fitted to the image.
Nowadays, with deep learning, far better results can be achieved.
This change from manual mathematical to deep learning-based models
has also started for occupancy grid mapping.
Referring to the old designation,
we call this approach deep inverse sensor models.
In the last video,
we used a geometric approach for occupancy grid mapping.
Now, I will present how this task can also be solved using deep learning.
In this case, our task is not to develop
the algorithm itself that solves the problem,
but we have different challenges.
The most important question is how to create training data.
For this task, we need samples
consisting of a LiDAR measurement and a corresponding “label” occupancy grid map
to teach our model what a correct occupancy grid map
based on this measurement shall look like.
You have already learned in previous units that,
for example for image segmentation,
we usually need labeled datasets,
so we need humans to label thousands of images
and annotate which pixel belongs to
the street, a vehicle, a pedestrian, and so on.
Imagine doing this for an occupancy grid map.
You, as a labeler, would see such a LiDAR point cloud
and would have to draw the correct occupancy grid map by hand.
This effort would be immense!
Several other approaches
to generate training data without manual effort
have been proposed.
One approach uses a simple geometric inverse sensor model
to create occupancy grid maps from LiDAR measurements as labels
to train a neural network to create such grid maps from radar measurements.
As LiDAR measurements are more precise,
the geometric ISM performs better than on radar measurements,
but, of course, it still has the same disadvantages
that were discussed in the last video.
Another idea is to create multiple occupancy grid maps sequentially
and fuse them into a denser grid map that is then used as a label for training.
Also, this approach relies on a geometric ISM in the beginning.
The third approach was developed at ika and also seems very promising.
We use a complex simulation environment
to simulate sensor data in a virtual environment
and get label grid maps for free,
because, in simulation, we know exactly
which cells are occupied and which are free.
In the following, we will dive deeper into this method.
In the left image, you can see a LiDAR point cloud
that is generated by the sensor model in the simulation software.
There are several other vehicles of different types
and pedestrians moving around.
The textures of the world model include physical material information,
for example their reflectivity.
The sensor model uses ray tracing and physics
to create more realistic sensor data in the simulation.
We placed a second virtual LiDAR sensor at the same position as the real LiDAR sensor,
but this sensor has 3000 instead of 32 vertical layers,
hence it creates a much denser point cloud.
In the simulation, we also have
access to information about the material that caused the reflection,
which is shown with different colors in this image.
This material information can be used to create an evidential occupancy grid map.
Free cells are shown green and occupied cells are red in this image.
Additionally, we know where all objects are placed in the simulation.
This is used to mark all cells that are occupied by a traffic participant
as occupied, not just the cells where reflection points are located.
With this setup, we can generate as much synthetic training data as we want
virtually for free.
Of course, this does not come without restrictions.
To ensure that a model that was trained with synthetic data
also performs well on real-world data,
we want the discrepancy between real and synthetic data
to be as small as possible.
The neural network has the best chance to understand the problem
if it is presented with diverse training data
showing a lot of different scenarios, vehicle types, obstacles, etc.
And we need to find a suitable network architecture
that is capable of modeling the problem.
It requires a minimum number of layers or parameters
to be able to find a set of parameters that can solve the problem.
On the other hand, we need to ensure that
the neural network does not learn the training data by heart,
which is commonly known as ”overfitting”,
but we want the model to understand the problem.
This can be controlled by the network architecture
and the way that data is preprocessed before it is fed into the neural network.
If you know how to get your training data,
you still need to design the architecture of the neural network:
how to transform the data into a tensor,
which types of layers to use,
how many layers, how many parameters,
whether we need skip connections or not, and so on.
A lot of effort is put into this by many researchers worldwide.
So, there is no need to reinvent the wheel;
rather, do a good literature review
and find solutions for similar tasks that can be used as a starting point.
You have already learned about the PointPillars network architecture
in the previous unit.
It is one state-of-the-art model for 3D object detection in LiDAR point clouds,
so why not adapt this for grid mapping?
The architecture comprises a feature encoding layer
that transforms point clouds into a tensor,
which is then fed into the CNN backbone of the network.
That’s what we also need, so we keep that.
However, the detection head predicts 3D bounding boxes that we don’t need.
So we remove that and replace it with an evidential prediction head.
This is just another convolutional layer
with an output of the same size and resolution
as the occupancy grid map to be predicted.
Two values are predicted for each cell,
which is evidence for the cell being free and evidence for the cell being occupied.
Using a rectified linear unit activation,
it is ensured that these values lie in the range between zero and infinity.
Using subjective logic,
belief masses can be computed from this evidence
to create an evidential occupancy grid map.
At this point, the network would be able to
predict a tensor that can be understood as an occupancy grid map,
but we still have to tell the training algorithm
what is a good prediction,
so we define a loss function that measures
how close the predicted grid map comes to the label grid map.
This loss function is then minimized during the training
to calculate new parameters.
With this, we have our neural network architecture set up.
Skip to the next video to learn how this network can be trained and evaluated.